{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "logical-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "packed-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "train_data = pd.read_csv(r'C:\\Users\\Angus\\Documents\\UTS MDSI\\Advanced DSI\\NBA Kaggle\\adsi_g5_kaggle_nba\\data\\train.csv')\n",
    "test_data = pd.read_csv(r'C:\\Users\\Angus\\Documents\\UTS MDSI\\Advanced DSI\\NBA Kaggle\\adsi_g5_kaggle_nba\\data\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "consolidated-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust data\n",
    "train_data_x = train_data.copy()\n",
    "train_data_x = train_data_x.drop(['Id_old', 'Id'], axis=1)\n",
    "train_data_target = train_data_x.pop('TARGET_5Yrs')\n",
    "test_data_x = test_data.drop(['Id_old', 'Id'], axis=1)\n",
    "\n",
    "# MinMax Adjust data\n",
    "scaler = StandardScaler()\n",
    "df_train_scaled = pd.DataFrame(scaler.fit_transform(train_data_x), columns=train_data_x.columns)\n",
    "df_test_data_scaled = pd.DataFrame(scaler.fit_transform(test_data_x), columns=test_data_x.columns)\n",
    "\n",
    "# train test val splits\n",
    "X_data, X_test, y_data, y_test = train_test_split(df_train_scaled, train_data_target, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "amazing-tablet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\angus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:10:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "# Set XGBoost model\n",
    "# Source: https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "\n",
    "clf = XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "entertaining-belarus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6761255265721695"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model ROC\n",
    "roc_auc_score(y_val, clf.predict_proba(X_val)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pharmaceutical-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter tuning\n",
    "\n",
    "\"\"\"\"learning_rate\", [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ),\n",
    "            (\"max_depth\"    , [  3 , 4 , 5, 6,  8, 10, 12, 15 ] ),\n",
    "            (\"min_child_weight\", [ 1, 3, 5, 7 ] ),\n",
    "            (\"gamma\", [0.0, 0.1, 0.2, 0.3, 0.4 ]),\n",
    "            (\"colsample_bytree\", [  0.3, 0.4, 0.5, 0.7 ] )\n",
    "            \n",
    "n_est = [int(x) for x in np.linspace(start=200, stop=2000, num=50)]\n",
    "max_depth = [int(x) for x in np.linspace(2, 20, num=1)]\n",
    "min_samples_split = [2,5,10]\n",
    "bootstrap = [True, False]\n",
    "class_weight = [None, 'balanced']\"\"\"\n",
    "\n",
    "random_grid = {'learning_rate':[0.025, 0.05, 0.10, 0.15, 0.20, 0.25],\n",
    "              'max_depth': [3, 4 , 5, 6,  8, 10, 12, 15],\n",
    "              'min_child_weight': [0.5, 1, 3, 5],\n",
    "               'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "              'colsample_bytree': [ 0.3, 0.4, 0.5, 0.7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "responsible-timer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\angus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:12:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100,...\n",
       "                                           reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.3, 0.4, 0.5,\n",
       "                                                             0.7],\n",
       "                                        'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2,\n",
       "                                                          0.25, 0.3],\n",
       "                                        'max_depth': [3, 4, 5, 6, 8, 10, 12,\n",
       "                                                      15],\n",
       "                                        'min_child_weight': [1, 3, 5, 7]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Validation the tune model\n",
    "clf = XGBClassifier()\n",
    "xgb_rcv = RandomizedSearchCV(estimator=clf, param_distributions=random_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "xgb_rcv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "israeli-thousand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, gamma=0.2, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.05, max_delta_step=0, max_depth=8,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_rcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "specialized-growing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7019407128906697"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model ROC\n",
    "roc_auc_score(y_val, xgb_rcv.predict_proba(X_val)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "distributed-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check against test set\n",
    "y_pred = xgb_rcv.predict(df_test_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "imperial-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission output\n",
    "preddf = pd.DataFrame(y_pred, columns=['TARGET_5Yrs'])\n",
    "submit = pd.DataFrame(test_data['Id']).merge(preddf, right_index=True, left_index=True, how='left')\n",
    "\n",
    "#submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "accurate-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out\n",
    "submit.to_csv(r'C:\\Users\\Angus\\Documents\\UTS MDSI\\Advanced DSI\\NBA Kaggle\\ANSubmit1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "powered-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upsample data\n",
    "# Source: https://towardsdatascience.com/machine-learning-resampling-techniques-for-class-imbalances-30cbe2415867\n",
    "# Import the resampling package\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "continental-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust data\n",
    "# Returning to one dataframe\n",
    "training_set = pd.concat([X_train, y_train], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "conscious-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating classes\n",
    "Plus5Y = training_set[training_set.TARGET_5Yrs == 1]\n",
    "NoPlus5Y = training_set[training_set.TARGET_5Yrs == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "taken-quick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: TARGET_5Yrs, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Undersampling the majority\n",
    "undersample = resample(Plus5Y, \n",
    "                       replace=True, \n",
    "                       n_samples=len(NoPlus5Y), #set the number of samples to equal the number of the minority class\n",
    "                       random_state=42)\n",
    "# Returning to new training set\n",
    "undersample_train = pd.concat([NoPlus5Y, undersample])\n",
    "undersample_train.TARGET_5Yrs.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "oriented-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data for analysis\n",
    "# Separate undersampled data into X and y sets\n",
    "undersample_x_train = undersample_train.drop('TARGET_5Yrs', axis=1)\n",
    "undersample_y_train = undersample_train.TARGET_5Yrs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "civil-springfield",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\angus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:48:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6724755409800074"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set XGBoost model\n",
    "# Source: https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "\n",
    "clf = XGBClassifier()\n",
    "clf.fit(undersample_x_train, undersample_y_train)\n",
    "print(clf)\n",
    "\n",
    "# Model ROC\n",
    "roc_auc_score(y_val, clf.predict_proba(X_val)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ordinary-stock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: TARGET_5Yrs, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oversampling the majority\n",
    "oversample = resample(NoPlus5Y, \n",
    "                       replace=True, \n",
    "                       n_samples=len(Plus5Y), #set the number of samples to equal the number of the majority class\n",
    "                       random_state=42)\n",
    "# Returning to new training set\n",
    "oversample_train = pd.concat([Plus5Y, oversample])\n",
    "oversample_train.TARGET_5Yrs.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "engaged-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data for analysis\n",
    "# Separate oversampled data into X and y sets\n",
    "oversample_x_train = oversample_train.drop('TARGET_5Yrs', axis=1)\n",
    "oversample_y_train = oversample_train.TARGET_5Yrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "beautiful-tiger",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\angus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:48:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6532879600788544"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set XGBoost model\n",
    "# Source: https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "\n",
    "clf = XGBClassifier()\n",
    "clf.fit(oversample_x_train, oversample_y_train)\n",
    "print(clf)\n",
    "\n",
    "# Model ROC\n",
    "roc_auc_score(y_val, clf.predict_proba(X_val)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "married-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "annual-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SMOTE package\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "seven-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesize minority class datapoints using SMOTE\n",
    "sm = SMOTE(random_state=42, sampling_strategy='minority')\n",
    "smote_x_train, smote_y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "saving-traffic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\angus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6573633199317571"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set XGBoost model\n",
    "# Source: https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "\n",
    "clf = XGBClassifier()\n",
    "clf.fit(smote_x_train, smote_y_train)\n",
    "print(clf)\n",
    "\n",
    "# Model ROC\n",
    "roc_auc_score(y_val, clf.predict_proba(X_val)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-fountain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
